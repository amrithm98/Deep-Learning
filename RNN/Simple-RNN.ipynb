{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input: first Character \n",
    "output : next Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Kafka txt Metamorphosis ...\n",
    "# Input+Prev_hidden --> hidden -->output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=open('kafka.txt','r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size=len(data)          # Full Data\n",
    "chars=list(set(data))        # Set of Characters\n",
    "char_size=len(chars)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(138417, 81)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_size,char_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to Make of vector with number of unique characters size. if character is an '\\n' , then vector is [1,0,0,0,......(81 chars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries for Index-Char Conversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'\\n': 0,\n",
       "  ' ': 4,\n",
       "  '!': 3,\n",
       "  '\"': 5,\n",
       "  '$': 7,\n",
       "  '%': 6,\n",
       "  \"'\": 8,\n",
       "  '(': 10,\n",
       "  ')': 9,\n",
       "  '*': 11,\n",
       "  ',': 13,\n",
       "  '-': 12,\n",
       "  '.': 15,\n",
       "  '/': 2,\n",
       "  '0': 17,\n",
       "  '1': 16,\n",
       "  '2': 19,\n",
       "  '3': 18,\n",
       "  '4': 21,\n",
       "  '5': 20,\n",
       "  '6': 23,\n",
       "  '7': 22,\n",
       "  '8': 25,\n",
       "  '9': 24,\n",
       "  ':': 27,\n",
       "  ';': 26,\n",
       "  '?': 28,\n",
       "  '@': 30,\n",
       "  'A': 29,\n",
       "  'B': 32,\n",
       "  'C': 31,\n",
       "  'D': 34,\n",
       "  'E': 33,\n",
       "  'F': 36,\n",
       "  'G': 35,\n",
       "  'H': 38,\n",
       "  'I': 37,\n",
       "  'J': 40,\n",
       "  'K': 39,\n",
       "  'L': 42,\n",
       "  'M': 41,\n",
       "  'N': 44,\n",
       "  'O': 43,\n",
       "  'P': 46,\n",
       "  'Q': 45,\n",
       "  'R': 48,\n",
       "  'S': 47,\n",
       "  'T': 50,\n",
       "  'U': 49,\n",
       "  'V': 52,\n",
       "  'W': 51,\n",
       "  'X': 54,\n",
       "  'Y': 53,\n",
       "  'a': 55,\n",
       "  'b': 57,\n",
       "  'c': 56,\n",
       "  'd': 59,\n",
       "  'e': 58,\n",
       "  'f': 61,\n",
       "  'g': 60,\n",
       "  'h': 63,\n",
       "  'i': 62,\n",
       "  'j': 65,\n",
       "  'k': 64,\n",
       "  'l': 67,\n",
       "  'm': 66,\n",
       "  'n': 69,\n",
       "  'o': 68,\n",
       "  'p': 71,\n",
       "  'q': 70,\n",
       "  'r': 73,\n",
       "  's': 72,\n",
       "  't': 75,\n",
       "  'u': 74,\n",
       "  'v': 77,\n",
       "  'w': 76,\n",
       "  'x': 79,\n",
       "  'y': 78,\n",
       "  'z': 80,\n",
       "  '\\xa7': 14,\n",
       "  '\\xc3': 1},\n",
       " {0: '\\n',\n",
       "  1: '\\xc3',\n",
       "  2: '/',\n",
       "  3: '!',\n",
       "  4: ' ',\n",
       "  5: '\"',\n",
       "  6: '%',\n",
       "  7: '$',\n",
       "  8: \"'\",\n",
       "  9: ')',\n",
       "  10: '(',\n",
       "  11: '*',\n",
       "  12: '-',\n",
       "  13: ',',\n",
       "  14: '\\xa7',\n",
       "  15: '.',\n",
       "  16: '1',\n",
       "  17: '0',\n",
       "  18: '3',\n",
       "  19: '2',\n",
       "  20: '5',\n",
       "  21: '4',\n",
       "  22: '7',\n",
       "  23: '6',\n",
       "  24: '9',\n",
       "  25: '8',\n",
       "  26: ';',\n",
       "  27: ':',\n",
       "  28: '?',\n",
       "  29: 'A',\n",
       "  30: '@',\n",
       "  31: 'C',\n",
       "  32: 'B',\n",
       "  33: 'E',\n",
       "  34: 'D',\n",
       "  35: 'G',\n",
       "  36: 'F',\n",
       "  37: 'I',\n",
       "  38: 'H',\n",
       "  39: 'K',\n",
       "  40: 'J',\n",
       "  41: 'M',\n",
       "  42: 'L',\n",
       "  43: 'O',\n",
       "  44: 'N',\n",
       "  45: 'Q',\n",
       "  46: 'P',\n",
       "  47: 'S',\n",
       "  48: 'R',\n",
       "  49: 'U',\n",
       "  50: 'T',\n",
       "  51: 'W',\n",
       "  52: 'V',\n",
       "  53: 'Y',\n",
       "  54: 'X',\n",
       "  55: 'a',\n",
       "  56: 'c',\n",
       "  57: 'b',\n",
       "  58: 'e',\n",
       "  59: 'd',\n",
       "  60: 'g',\n",
       "  61: 'f',\n",
       "  62: 'i',\n",
       "  63: 'h',\n",
       "  64: 'k',\n",
       "  65: 'j',\n",
       "  66: 'm',\n",
       "  67: 'l',\n",
       "  68: 'o',\n",
       "  69: 'n',\n",
       "  70: 'q',\n",
       "  71: 'p',\n",
       "  72: 's',\n",
       "  73: 'r',\n",
       "  74: 'u',\n",
       "  75: 't',\n",
       "  76: 'w',\n",
       "  77: 'v',\n",
       "  78: 'y',\n",
       "  79: 'x',\n",
       "  80: 'z'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_int={ch:i for i,ch in enumerate(chars)}\n",
    "int_to_char={i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "char_to_int,int_to_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector for a single character 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_for_a=np.zeros((char_size,1))  #(81,1)\n",
    "vector_for_a[char_to_int['a']]=1      # [0,0,0,0,0,0,0,0,0,0.....1,0,0,0,0,0(81 Chars)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-1 \n",
    "hidden_layer_neurons=100\n",
    "sequence_length=25      # In one time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialising the Weights And Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input-Hidden\n",
    "Wxh=np.random.randn(hidden_layer_neurons,char_size)*0.01   #Transpose of what is expected 100X81 Single I/P: 81X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 81)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wxh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recurring Weights...Hidden-Hidden\n",
    "Whh=np.random.randn(hidden_layer_neurons,hidden_layer_neurons)*0.01   #Transpose of what is expected 100X100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Whh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hidden-Output\n",
    "Why=np.random.randn(char_size,hidden_layer_neurons)*0.01   #Transpose of what is expected 81X100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81, 100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Why.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hidden Layer Bias\n",
    "bh=np.zeros((hidden_layer_neurons,1))\n",
    "#Output Bias\n",
    "by=np.zeros((char_size,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(inputs,outputs,previous_hidden):\n",
    "    \n",
    "    xs,hs,ys,ps={},{},{},{}\n",
    "    \n",
    "    hs[-1]=np.copy(previous_hidden)\n",
    "    \n",
    "    loss=0\n",
    "    \n",
    "    #Forward Pass\n",
    "    \n",
    "    for t in range(len(inputs)):\n",
    "        \n",
    "        xs[t]=np.zeros((char_size,1))\n",
    "        \n",
    "        xs[t][inputs[t]]=1\n",
    "        \n",
    "        #print(Wxh.shape,xs[t].shape,Whh.shape,hs[t-1].shape,bh.shape)\n",
    "        hs[t]=np.tanh(np.dot(Wxh,xs[t])+ np.dot(Whh,hs[t-1]) + bh)\n",
    "        \n",
    "        ys[t]=np.dot(Why,hs[t])+ by\n",
    "        \n",
    "        ps[t]=np.exp(ys[t])/np.sum(np.exp(ys[t]))\n",
    "        \n",
    "        loss+=-np.log(ps[t][outputs[t],0])\n",
    "        \n",
    "    #Backward Pass\n",
    "    \n",
    "    dWxh,dWhh,dWhy=np.zeros_like(Wxh),np.zeros_like(Whh),np.zeros_like(Why)\n",
    "    \n",
    "    dbh,dby=np.zeros_like(bh),np.zeros_like(by)\n",
    "    \n",
    "    dhNext=np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(xrange(len(inputs))):\n",
    "        \n",
    "        dy = np.copy(ps[t])\n",
    "        \n",
    "        #dLi/dfk=pk-1\n",
    "        dy[outputs[t]] -= 1\n",
    "        \n",
    "        dWhy = np.dot(dy,hs[t].T)\n",
    "        \n",
    "        dby += dy\n",
    "        \n",
    "        dh = np.dot(Why.T,dy) + dhNext\n",
    "        \n",
    "        dhRaw = (1-hs[t]*hs[t])*dh\n",
    "        \n",
    "        dbh += dhRaw\n",
    "        \n",
    "        dWxh += np.dot(dhRaw,xs[t].T)\n",
    "        \n",
    "        dWhh += np.dot(dhRaw,hs[t-1].T)\n",
    "        \n",
    "        dhNext = np.dot(Whh.T,dhRaw)\n",
    "        \n",
    "        for val in [dWxh,dWhy,dWhh,dbh,dby]:\n",
    "            np.clip(val,-5,5,out=val)\n",
    "            \n",
    "        return loss,dWxh,dWhh,dWhy,dbh,dby,hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSentence(hidden,index,length):\n",
    "    x = np.zeros((char_size,1))\n",
    "    x[index] = 1\n",
    "    \n",
    "    generated_text = []\n",
    "    \n",
    "    for i in xrange(length):\n",
    "        \n",
    "        hidden = np.tanh(np.dot(Wxh,x) + np.dot(Whh,hidden) + bh)\n",
    "        y = np.dot(Why,hidden)\n",
    "        p = np.exp(y)/np.sum(np.exp(y))\n",
    "        \n",
    "        next_index = np.random.choice(range(char_size),p = p.ravel())\n",
    "        \n",
    "        x = np.zeros((char_size,1))\n",
    "        x[next_index] = 1\n",
    "        \n",
    "        generated_text += [next_index]\n",
    "        \n",
    "    \n",
    "    gen_text = ''.join(int_to_char[i] for i in generated_text)\n",
    "    print(\"\\n\"+gen_text+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q.PKB1cL@(;\n",
      "z4)v!zmD4vfpxFA/Sonq�X8Dnxw93Pin (qb3jb;@gYENGCK\n",
      "6bK(4Kv('ocL%uNbq*zkM%)h@w-wVywbjXa%�L@3EiEf8mex�;Mx49Iom. D8$�yt\"U:.WW?\n",
      "k;K;qYEj7L*1nnF\n",
      "ag%cgxkA!s%cUkCqHw;B*ql:�ypO)ay�nOt74ioo-,6cbCCg/U\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hprev = np.zeros((hidden_layer_neurons,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "generateSentence(hprev,char_to_int['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_steps):\n",
    "    \n",
    "    p = 0\n",
    "    mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    mbh, mby = np.zeros_like(bh), np.zeros_like(by)    \n",
    "    \n",
    "    smooth_loss = -np.log(1.0/char_size)*sequence_length                                                                                                                       \n",
    "\n",
    "    loop = 0\n",
    "    hprev = np.zeros((hidden_layer_neurons,1)) \n",
    "    \n",
    "    while(loop <= n_steps):\n",
    "        \n",
    "        if(loop == 0 or p+sequence_length+1 >= len(data)):\n",
    "            hprev = np.zeros((hidden_layer_neurons,1)) \n",
    "            p = 0\n",
    "            #Reset Memory at the beginning and end of one set of training data\n",
    "            \n",
    "        inputs = [char_to_int[i] for i in data[p:p+sequence_length]]\n",
    "        targets = [char_to_int[i] for i in data[p+1:p+sequence_length+1]]\n",
    "        \n",
    "        #print(hprev.shape)\n",
    "        loss, dWxh, dWhh, dWhy, dbh, dby, hprev = loss_function(inputs, targets, hprev)\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            \n",
    "        if(loop % 1000 == 0):\n",
    "            print(\"Iteration: \"+str(loop)+\" Loss: \"+str(smooth_loss))\n",
    "            generateSentence(hprev,char_to_int['a'],200)\n",
    "            \n",
    "        #Adagrad Update\n",
    "        for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "            \n",
    "        p += sequence_length\n",
    "        loop += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Loss: 109.813920252\n",
      "\n",
      "iueg oodk rk bten eomret and iinniGsee coredgded uliyrererdeesgrydtein ie te ifs hedenyet yy eedoe. rus al nlv,or,el'itloooejrend inad Wy Gre toncoule)\n",
      "er thereriiaw das nidenour fyofxdverishejs er th\n",
      "\n",
      "Iteration: 1000 Loss: 94.9887224358\n",
      "\n",
      ", d\n",
      "iuxg,,Tewt.ohes,yg,yg,ftpholvhegen yihcoueer wemhash\n",
      "v cac,mh,vcvs s1rghevh,oekthotrvcfvefhrincgh,Stcvh\n",
      "v\n",
      "uvhegIvaDa\"cincghw\"hkgheGlvhonmr- I?G th \"hivhxvh,? ,hheh,v \n",
      "gn,gn hcvhc?ccmeug,efdaw\n",
      "?TaT\n",
      "\n",
      "Iteration: 2000 Loss: 81.2229400425\n",
      "\n",
      "djH hB 'PServyevxigiamaxnd,manddtGc;Tat?.;vasvo b0T0h Ne fheao,,janiee tomn \n",
      "%kB�6gothirO\"gj,adBinanj sBropt,dcean\n",
      "UnQg,giIcnma cTv t Trcuacc,jt \n",
      "iGrie hil.\" kol jlem.pim\n",
      "0rje.,us wy 0ed vH, -v\"i;gs\n",
      "Y\n",
      "\n",
      "Iteration: 3000 Loss: 73.9952140641\n",
      "\n",
      "cu cn wnereiccx, Grceo;crecH Gi-q;ion. wor;i;;y.ct;f,;r \n",
      "u\n",
      "er.,i\n",
      "sbl-\n",
      "hhoxnGlgoeloch fsv;g\n",
      "lingve, chaHnq;clcursydah,h kavxrBFr; G;irre,fve'n;ywfordnmmv;hisimhr,, w\"vm'ns Whifs,;y,; \n",
      "Reum\n",
      "A \n",
      "t \n",
      "hitwiu\n",
      "\n",
      "Iteration: 4000 Loss: 70.2371276363\n",
      "\n",
      "lPnhinemarf dhum?eaI \n",
      "Acyadiv haty forYordier boughommfedt hiirx,y fiigacx winr aSgos, on Wod aedlkilas tos bi t\n",
      " oveiAsiTe,verindongp -ror?eir he\"cor t, mosthincsregipIton codu bh fydy vx whe rron\", \n",
      "\n",
      "Iteration: 5000 Loss: 72.0216489008\n",
      "\n",
      "ecceecmigx Lu)Drgivelentlirg.TCasf. Rir?gdeugiteer fe Goole Gr,. \"Eng, Dacid Radpbufrjer \"1lsanh gs prkhec\"d,G., \n",
      "Racbiobith Gt\n",
      "Eicsilethv,jic\n",
      "xroy )vep\"dhe\"nrfT.g uliviregrih1his\"tiLn(ktrpcnug Pug ia\n",
      "\n",
      "Iteration: 6000 Loss: 74.1460489839\n",
      "\n",
      "fueglagJa xn , Tcmiuole.n.\n",
      "GAdug Ggds, pf f\"mpfune) waogloflRg ilc.an\"pvi , \n",
      "uunherfctuithho, chir tG Wif LPRYYy nn. cistinte fom'or 5Fqft, gave\n",
      "wace ricn th, Gt ur G5va.\"U3in.n?5UD1e ,tiwgo\n",
      "Gs\n",
      "sd fjl\n",
      "\n",
      "Iteration: 7000 Loss: 69.3506887378\n",
      "\n",
      "ffUfQher Tos arhe tA.,.go'yob\n",
      "c, ver;\n",
      "\n",
      "\n",
      "kaog N\"iGoxgmd \n",
      "terms Wk,oml;ia;n, mactiHyut.M b)uHit,ungsegTiDo ann ve f5the kg, j c0s;\"ysefm \n",
      "Wav jvef ofe j'j inb fmD brat irEde M?cHaf the.cGce touset.fi.uj\n",
      "\n",
      "Iteration: 8000 Loss: 66.290297528\n",
      "\n",
      "tC Ghe eftrifserdh\" whe lT chor s aW, nt\n",
      "Grer, woumtip \n",
      "xeawHnuot mhe, qlrYvve,, \n",
      "wns fhitilg,l, ,mel \n",
      "fPisevRrkceer, tafremr Wprlfqm?v,evNfa?cTuame ls Gocer tometer5e fis th'rac, Gycacyre)g, dailel i\n",
      "\n",
      "Iteration: 9000 Loss: 65.0049232505\n",
      "\n",
      ";thancarny\"; \n",
      "\"taClendgithig?c\"teginxhh\"n;\n",
      "Maches c, meGg; wadgrr \"f;dy'imy\"xfuwe!o\n",
      "fvipxndlgorf, orsjw bMf therg,f, \n",
      "Hisedir\n",
      "hEpqL \"ouRe Gr?afl oc,,or aren,erkcer, is ynEor Gxs,my som ad M. migYtoskv\n",
      "\n",
      "Iteration: 10000 Loss: 64.442329527\n",
      "\n",
      "n wiogegheagFy\n",
      "by,egdg FoAd,\"G M.m cicy ce stey\",uBg abv rpy,or, enk,-\"thecorod, t' gac cUugetef\n",
      "aTpnt'dh,vanth;rcwedjy srvisen wheceg\"yuva.yreceomding bsomwahken seme,utsos\"c,adawerrle hetis -ubu aHY\n",
      "\n",
      "Iteration: 11000 Loss: 73.3616113939\n",
      "\n",
      "I.7s?ELbERu!xpCU ytpiAdhefD-t urkisMdplEen thid RU1rt,T. joc, thivG\",dJc-lBiiGcucengilrJafrincRCh Purso\"M fut\"'Easoskin.\" fhy?erceeg,uvyr\"., LrotfDiTGaniecy\n",
      "9Fmi1m)IGrA5\";P,m.eiEg ivhI;tmut, cTimevTiE\n",
      "\n",
      "Iteration: 12000 Loss: 69.1704179926\n",
      "\n",
      "nrevilngtxonv?t;off, uu()3oRfnden.\"056uThk Ileyxlipf nvimkuw'S Ptegtev.,or\n",
      "wrefrices, 1Isilit0mis \n",
      "B1y arsevvesk,; Twi\n",
      "y cd hiny ag Aoulingou gJTcotomoit Pvau0f.oulnw\n",
      "haiud *Fr GrD.hhoullc.S.c\"t toamo\n",
      "\n",
      "Iteration: 13000 Loss: 65.3414232337\n",
      "\n",
      "n 1ecubee Dofing \n",
      "xlkA!.seeg, thicm\n",
      "jusmEent, Lp) So1y ho hous, RTfad! chanr hein;gv;oIecAy,ilsd ris fu\n",
      "j oCqs WUe, voy, Tnxw.d toud WT hamt\n",
      "t;abarev jBp;tom omyingGv IGme whas, Wakt \"ve kMoe\" was tha\n",
      "\n",
      "Iteration: 14000 Loss: 63.5514094017\n",
      "\n",
      "ug; Lhk; dv.\"jmWess;E n\", Fh,  hor'elxw ei; brecMs UnA)HiGdxM.T\n",
      "jor\"?elwbevef.oufheIw, heg.,touyricff hi5r\n",
      "Hld.-?';*C., hapGxof) wsocevvinrhef chad -hangt;o, hasGWft'cee woe Bme wafxpiRd, fof cacn;. \n",
      "\n",
      "\n",
      "Iteration: 15000 Loss: 62.7742721068\n",
      "\n",
      "nd; me \"vempanE,, cikm fecar h?\"*ces; wo sovl, \n",
      "ffheuj diyk,,mhermeveiPsmthingoonglm fomy, me,mlD w mhe cch, ghiacomed prer, LTund.wyhemjetk, ;\n",
      "SrcGu,t xres hacegheug \"IcanPnv,ank, Gre wwEaclir rre;, \n",
      "\n",
      "Iteration: 16000 Loss: 65.8343713468\n",
      "\n",
      "-ttmidr.,?\n",
      "ev,iff Lqong, hSind m'nl PLa-; -rtror?pos Grtithin'd Gr, cStha,,'n, Gt GC. Tf*'av);me?fB ce hoennox;piiby,, tre bitmourevj\n",
      "wAcof bec.ktinv?;esml\"ffM shveunen efungevey, mTEW,unt Gr*? En.. h\n",
      "\n",
      "Iteration: 17000 Loss: 70.141676861\n",
      "\n",
      "nplL turls.PuIm\n",
      "Wuvu.cThongriug, uf fav0me;,\"othilUge.l?hke?s, he9mev\"D\"50u-Fk.B\"SEI495H.,inti\"ivE. Lowed\"? let biRedu\n",
      "HNhi\n",
      "wavirk.unes bTE;ktG$'iI s.ouG 30xEwhouedrenc�y Grd, �SEE\n",
      "L\n",
      "oullte fY,mayd,ce\n",
      "\n",
      "Iteration: 18000 Loss: 66.0009492338\n",
      "\n",
      "qegyf. t\n",
      "tome oICcaben br houl rheever.?anglol;-r dheg bf jfe\"'v GoloviyivecB\"!be-m?cac ficGsyho'y sr.,hackepsovhe \n",
      "qYicl,oflevc 've Iitjipn Hf *very Gbwl 1E.*vr,;aHU)yjykang \n",
      "b? )ryea,yoteru/gr;sy io\n",
      "\n",
      "Iteration: 19000 Loss: 63.0807104286\n",
      "\n",
      "ngC, botlc\".Ueygy Fuc, sket.\n",
      "TC.; sobtheingalsDmeg )ecekc\n",
      "y;,,s \"prr;, W1cbrd thid asea\n",
      "Gr sovodhiving!ln.nng NoLpevine.co\" bl\n",
      "arilghac f why r rws,chen, hi;nSviy, his, fukl?ey weDkes!ls, \n",
      "kel,h sic f\n",
      "\n",
      "Iteration: 20000 Loss: 61.9367201791\n",
      "\n",
      "af sled;ocsel. go 3hme'n'nd,lerk Gvkor,, maseevt'atly'pand; HTMoyk, wivkr;E.\n",
      "iba Tomkis; \"Sfmo\n",
      "qctor\n",
      "Uag \n",
      "His; ofacb) ark'y nna.\n",
      "Rr W'm,e!;orel\n",
      "Ialm, bocheifiw)r)tvert mer, thanbcAjNTnomul s@ cithomta\n",
      "\n",
      "Iteration: 21000 Loss: 61.5779442898\n",
      "\n",
      "c!ryvet,iv IpcRimmd,, macgereclache\"j; sl\n",
      "WTb;\n",
      "jfriul lmed-git \n",
      "Gc(? hillvonjd da5dkouve tott alettalgomeditjmi\"dpd;; Wovilke uvibe \n",
      "C!rve Fnd, andeng bes ropl\"?'s., oram, Gs angp,,er, whed \n",
      "qDkenad,,\n",
      "\n",
      "Iteration: 22000 Loss: 70.4179877196\n",
      "\n",
      "nd..,min\".ofbJfr;ELWouklipaJwi\n",
      "ling,o cq bo, nid beDcDkRr?Wx;poue'miMJR)1! AEGxgifthafrt.cTkuc, wLulC\"pwirPed, p5cs Sfkang, GingagIafsvMhMvkem,.rkxU?fC,w-\"0Rj0- thath,,)av. fC wApm.,xU!d,,)th,,aecDvat\n",
      "\n",
      "Iteration: 23000 Loss: 67.1147342606\n",
      "\n",
      "r \n",
      "woms ouR�e'JYm T\n",
      "1evyo;0;\"?wJefofGncent, mec ansmE, leRpoa,f Hv bacliwgo., MAofo\n",
      "Ukillky EAlutcics mo-sen, laNnjeytev-Mel.? eDWavkgouivderig Gr WimaTc\n",
      "GId6meening.\"? %aG -CGn3,a?c.ivh -GR;urd;poo)m\n",
      "\n",
      "Iteration: 24000 Loss: 63.3225838918\n",
      "\n",
      "pn,ov;,o\"of, woou, cyind,t,- \n",
      "EaFung Lavecso, ifhed wimenojim's;i,f, )R, Lpem.sk,ceD; baGf\"DHhim mimTsy jis\n",
      "mecag his.is \n",
      "ond \n",
      "ic, wory bic.0 TN \n",
      "qCetiuB wus'.d.\"H0heudb s,s waEIlG 7cher)eif wendry bi\n",
      "\n",
      "Iteration: 25000 Loss: 61.3136865739\n",
      "\n",
      "lmGthein,ng\"Mbe,;,inndeg WudedncIWime so vinc hut timing\n",
      "TB LevjegH bH ;ocwew,, \"cxs)enciy,gaf baddt;oucBay.roth, Greg Bn'theos uGj, \n",
      "vironv;ob;s, ch bbd\";fiv,,ec cJm lHy\n",
      "\n",
      "qussibiver be, pu\n",
      "Nkmin,.,, \n",
      "\n",
      "Iteration: 26000 Loss: 60.7117340585\n",
      "\n",
      "f bo\"wguud bufis \n",
      "acle caf hery,,.,inc .hacgucapbe; \n",
      "keoc\n",
      "uq,f, bvel.\n",
      "\"acou;, redmicdaathed.je Tpr\n",
      "wexc, chuhe whav henmouco,u'f;, gl\n",
      "sac,o bN hasl?e0c her of :ot ale pung,'eg'sC FI@lareg ghD,;.,, tBm\n",
      "\n",
      "Iteration: 27000 Loss: 63.3830955866\n",
      "\n",
      "ntre som, sDciy poulev??, WoF Lvas colJd;vDi begot\n",
      "RselwyofF\n",
      ",\n",
      ")l bnuicer, into'Js Gr Tbe'0iuckiflhe')tanvprouseh\",tivichag anNy bi5n'byy bas,,\"., bvangit tha, d dJR, jr bpiGnexd., ST'sa\", kictre bpuc\n",
      "\n",
      "Iteration: 28000 Loss: 68.8659097169\n",
      "\n",
      "vork,\n",
      "pe3cmaR3dofp\"coup. Toft tavivetin\"o\", Evidupn, cuNpy;are \n",
      "kC 5veof.et, ipu\"AbintloulrteoDg acJaog Gicelg ha-gm;'y TsxlaR. tootfey,,, en,,,, oos \"r,em.UkxR6ey! Seckua'xuven'd mheg,or?,rinHf.\"MYta\n",
      "\n",
      "Iteration: 29000 Loss: 64.7259808905\n",
      "\n",
      "xed\n",
      "jkd ofcadt GxR\"ur\n",
      "\"0gn cTERexco 3as* t\n",
      ".ef fedind, hou 1kl.\"k,p,'MGE\n",
      " 3Crtusighed houd?mem, PWrevk\n",
      " umD caldtidkxy fere Pnly Werevoualgve govegictr GTkicr Livat TuR fmcofo Tsis3b. ibe!D TRf.s; hr \n",
      "\n",
      "Iteration: 30000 Loss: 61.7419490295\n",
      "\n",
      "verachaoff ifo, \n",
      "u�n Pluvacf,ishind -?s; Htelfo\") ess mork?Ukhocmi- ;n TMycacE5.?;f Wipjfd?reb ;Rthe houqC;, hencivegevk,.\n",
      "\n",
      "aleg'bsj, Hon hir, fucegt?er;\n",
      "isilS,?.'Bs wiung Grevit?eve fomeve; wixck, wo\n",
      "\n",
      "Iteration: 31000 Loss: 60.6029323421\n",
      "\n",
      "dexjed kar.gimns, \n",
      "roumis\"jADd .Mckmum Gfg eadt;irk,.ciache joul3; and \"Sqildag, fucjdor, jting,ugvy cisg,;, wfc, \n",
      "HRE g 1m?*b'd citiwher;\n",
      "joumed;;, hoR0-wouRsanlpelin Te bofg\"yMnlD-thyDsveb\n",
      "tavine be\n",
      "\n",
      "Iteration: 32000 Loss: 60.3402486562\n",
      "\n",
      "cdevep ind., Wrixf avpedige\n",
      "R, thicFog inng?)h!s,-aHs;-onY3\"tse, hlomeverJing Grg aBd,, ro heafe;g; TPliclas hever.ike;d;ne, \"As0imd, af TWiinouUdukncif mecounly ared'cbed,r\n",
      "Dbm.gingme) GIRl.nothio AR\n",
      "\n",
      "Iteration: 33000 Loss: 68.7325199056\n",
      "\n",
      "ve*wer;iLEJ0athun?E?Dam;\n",
      "0MltDomk, Gud;lE)MD.re\"tis;; \n",
      "TsfCpid gCer, LE\n",
      "Her\n",
      "ovkobe\n",
      "RL RKlyCpRneD fpep, 1D\"M;cme. rorve bim,5l?)�,,bice cDonviTMfangun.;.\n",
      "WDrDr F)kf jut!Dab,EL,)e rTrJ UE1J,,Em)\n",
      "FREor)i\n",
      "\n",
      "Iteration: 34000 Loss: 66.0690656209\n",
      "\n",
      "f'pe fj MpfpumRgoulg thak;gace, bvGd.;.D. .idn, tisee,\"?DjRJD?\" \n",
      "-1y Pnd, \n",
      "?icmum?P arx. \"ARupfE Mm'og, ofpe \"zjJ.\n",
      "-Rcoufxum,eicqrm\n",
      "Fb guCJf Pt muikof, evahis.Faldd pnts?;\n",
      "Uve kirg Lagcen.?\n",
      "ocobedm;k.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 35000 Loss: 62.4223079071\n",
      "\n",
      "teod.,,, LHcofFtucCet;weR)t shiy sxUt loD fnthDtuDg0 o0,d\n",
      "\n",
      "FT'v? bace Padel\"r,,, pe\n",
      "s;c, we?\" N,Djb.mabg \n",
      "fokd \"faw.-ny, -orex\")?en,,, ves.yejc.t hodeng, \n",
      "H-, wooves, ck Iust \n",
      "H,it Gr \"senc\"HY'ongecsa\n",
      "\n",
      "Iteration: 36000 Loss: 60.3411133693\n",
      "\n",
      "d bGcigong, \"qU0;\n",
      "whadcC?\")gawlsiigirnd giR RaUgont; hicqpus;inct vat, thotf leg.thecer\n",
      "\n",
      "Rmilg,hirc\" ied c-eKs,,, be1; w\"j cwavevqMekc, hanW 1cRedntDf wevt copinftDegs the..,\n",
      "thlalndum,, av,ihig jf Gc\n",
      "\n",
      "Iteration: 37000 Loss: 59.5732874839\n",
      "\n",
      "nvn Hei.?e hive Gfuelagef; aCcas chen\"tYGr cGs., bind Gr.r blrig!edtefj)e.,), byd UGr?IP soG ?u3ly Fasga-f, Waxj Gr!chos ,xjFvegixgeur, Ladg,,, her, Gneg;; \n",
      "hickung thou'bn \n",
      "Cmoge ant,;,, inx\"w, Mf ju\n",
      "\n",
      "Iteration: 38000 Loss: 60.7857432958\n",
      "\n",
      "recuicJuls iHv, kicmm\"t, whengnghuy\"of., \"vere.Favef;s M he ;ny \n",
      "U�7.C\n",
      "LBum Gx-ojd tDed \"0U carcmeE*\n",
      "Has qucmeats,y, Ne hbo0'df qABJfanged foulGl.\"; Gfsan cickin'sal tD \n",
      "Sor0,  TTiacqnyx.ss; mCibl tov\n",
      "\n",
      "Iteration: 39000 Loss: 68.4273727184\n",
      "\n",
      "3vor Fughece,cfr)im hCree whe LI\n",
      "wIN'ung M)ch, woulC ulthe)mioj;ias unE)\")F\"5CcALe nb SPp0mise\"pMiwNechELjicer5iront'Eg.)wGr;.t\n",
      "G0fs\n",
      "LNrenRiEhe, sie0R,Cp\n",
      "g,?\"RE*?iMc qMacex'ge ferjr;icev,ibe Bcadj\"\n",
      "Iu\n",
      "\n",
      "Iteration: 40000 Loss: 64.2248121578\n",
      "\n",
      "sJ :habed holncigrea\n",
      "toul.d.UJs.MLERxwhug sufJ\n",
      "Gr Lrpang mud backic sus Th1 LowEfroriMec\"om, tis \n",
      "ion\" Xind;, Dins i0 lhDUj incog. vorkof stinghe'PhL Yheo\n",
      "T1y7,,.;ouly McDey. Gev;t; T sqong,\n",
      "fy \"cfe n\n",
      "\n",
      "Iteration: 41000 Loss: 60.9117836339\n",
      "\n",
      "ctibn!os cure?k \n",
      "pivherer\n",
      "Lual;; yif heg Tivome \n",
      "Rucos,rome jris Fd Wis;iictaler hATingy WaNg, averur,, whith bl\"\"\n",
      "f;udCr5d cialsh?s0r, L\"qr, y, Lvimi-. Gr av; aifmengom\n",
      "bUUJ'd.acksen0?)Eive\"jm,RT ho \n",
      "\n",
      "Iteration: 42000 Loss: 59.6731655565\n",
      "\n",
      "njle?\"5c;hancholgjr, hivevind, ofsjeingd eve borcviss, and, svort,,cex\n",
      "NopqhyYavjeuc,E)lceve pimaver 1bd ,CEome,,imsomker byy!orE.regs,;, sec, lidg;ef.0;omf-Gry-BoN,keecyavr Gbukt,y; H3thotrer'rHof.,J\n",
      "\n",
      "Iteration: 43000 Loss: 59.4773437167\n",
      "\n",
      "thty Roupud,;, on\"3C Gred?, wevev tiisd,\n",
      "\n",
      "cThily.ify sired,, guageibly 0rcto lldD Gf.-j;, Gr radker\n",
      "jithel? IRk\")Y. ff ghared, Gr, \n",
      "Hom. \n",
      "eug -nye GNd,imteu qf Tf hTh)iwe;0U TTfiviwhocle qfunt AoulCdc\n",
      "\n",
      "Iteration: 44000 Loss: 66.6117144214\n",
      "\n",
      "sbJ.\n",
      "y GITeckas \"PrubecbIE., Miek,vuyereyteng Lpk.n\"E*pr, Eroas jichuy)y, \n",
      "TRE; \n",
      "LU\"\"!sLIkug omCk!att kin.RIEJFpey,rickive\"0theup Rave,,, chofveld WY5y bacJjk cjarlo. GT.1.E*Let;,;, Mradgwjp,;,\"Dt.chi\n",
      "\n",
      "Iteration: 45000 Loss: 65.2958050842\n",
      "\n",
      "vorti dou0inwBiid 5ev, whGRd\")I \n",
      "Tcamk ond.-thr.\n",
      "Grvave \n",
      "liplljad kulthad 5rind,d.F--y,pin5ng ef rudmorlMm; Nf thiveang\" MNabulcom faryEHA-silk GIbang!a-xngachinp1Defack GARgr 1oL Dving av otEolsn.)M;\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-62dfd7ee6e68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-1dc8f84d74a6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_steps)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m#print(hprev.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-2ec46ff84258>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(inputs, outputs, previous_hidden)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m#print(Wxh.shape,xs[t].shape,Whh.shape,hs[t-1].shape,bh.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
